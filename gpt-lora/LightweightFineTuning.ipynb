{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRA\n",
    "* Model: gpt2\n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft                      0.5.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep \"peft\" || pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [\"train\", \"test\"]\n",
    "dataset = {split: load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=split) for split in splits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 8530\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 1066\n",
       " })}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def compute_stats(dataset, subset_name):\n",
    "    \"\"\"\n",
    "    Compute and print statistics for a subset of a HuggingFace dataset.\n",
    "    \n",
    "    :param dataset: A Hugging Face dataset object.\n",
    "    :param subset_name: A string with the name of the subset.\n",
    "    :returns: A dictionary containing the computed statistics.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        subset = dataset[subset_name]\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Subset '{subset_name}' not found in the dataset.\")\n",
    "\n",
    "    if 'text' not in subset.features or 'label' not in subset.features:\n",
    "        raise ValueError(\"Dataset must contain 'text' and 'label' columns.\")\n",
    "\n",
    "    sentences = subset['text']\n",
    "    labels = subset['label']\n",
    "    \n",
    "    # Number of samples in subset\n",
    "    num_samples = len(sentences)\n",
    "    \n",
    "    # Sentence length statistics\n",
    "    sentence_lengths = np.array([len(sentence) for sentence in sentences])\n",
    "    \n",
    "    # Labels statistics\n",
    "    label_counts = Counter(labels)\n",
    "    unique_labels = list(label_counts.keys())\n",
    "    label_percentages = {label: (count / num_samples) * 100 for label, count in label_counts.items()}\n",
    "    \n",
    "    # Compute statistics\n",
    "    stats = {\n",
    "        'subset_name': subset_name,\n",
    "        'num_samples': num_samples,\n",
    "        'max_length': int(np.max(sentence_lengths)),\n",
    "        'min_length': int(np.min(sentence_lengths)),\n",
    "        'mean_length': float(np.mean(sentence_lengths)),\n",
    "        'median_length': float(np.median(sentence_lengths)),\n",
    "        'unique_labels': unique_labels,\n",
    "        'label_counts': dict(label_counts),\n",
    "        'label_percentages': label_percentages\n",
    "    }\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Statistics for {subset_name} subset:\")\n",
    "    print(f\"Number of samples: {stats['num_samples']}\")\n",
    "    print(f\"Sentence length - Max: {stats['max_length']}, Min: {stats['min_length']}, \"\n",
    "          f\"Mean: {stats['mean_length']:.2f}, Median: {stats['median_length']:.2f}\")\n",
    "    print(f\"Labels: {stats['unique_labels']}\")\n",
    "    print(\"Label distribution:\")\n",
    "    for label, percentage in stats['label_percentages'].items():\n",
    "        print(f\"- Label {label}: {percentage:.2f}% ({stats['label_counts'][label]} samples)\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for train subset:\n",
      "Number of samples: 8530\n",
      "Sentence length - Max: 267, Min: 4, Mean: 113.97, Median: 111.00\n",
      "Labels: [1, 0]\n",
      "Label distribution:\n",
      "- Label 1: 50.00% (4265 samples)\n",
      "- Label 0: 50.00% (4265 samples)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'subset_name': 'train',\n",
       " 'num_samples': 8530,\n",
       " 'max_length': 267,\n",
       " 'min_length': 4,\n",
       " 'mean_length': 113.97162954279015,\n",
       " 'median_length': 111.0,\n",
       " 'unique_labels': [1, 0],\n",
       " 'label_counts': {1: 4265, 0: 4265},\n",
       " 'label_percentages': {1: 50.0, 0: 50.0}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_stats(dataset, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ace0edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for test subset:\n",
      "Number of samples: 1066\n",
      "Sentence length - Max: 261, Min: 14, Mean: 115.52, Median: 113.00\n",
      "Labels: [1, 0]\n",
      "Label distribution:\n",
      "- Label 1: 50.00% (533 samples)\n",
      "- Label 0: 50.00% (533 samples)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'subset_name': 'test',\n",
       " 'num_samples': 1066,\n",
       " 'max_length': 261,\n",
       " 'min_length': 14,\n",
       " 'mean_length': 115.5234521575985,\n",
       " 'median_length': 113.0,\n",
       " 'unique_labels': [1, 0],\n",
       " 'label_counts': {1: 533, 0: 533},\n",
       " 'label_percentages': {1: 50.0, 0: 50.0}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_stats(dataset, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8808d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4aa3876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "90506af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85d768c226746bebf1b8fc26b2d3905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(examples): return tokenizer(examples[\"text\"], padding=True, truncation =True)\n",
    "\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(preprocess, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e71e4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained('gpt2',\n",
    "                                                      num_labels=2,\n",
    "                                                      id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "                                                      label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1})\n",
    "\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "for param in base_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5042ed1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2dcb8def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_prediction):\n",
    "    predictions, labels = eval_prediction\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f90aef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_output\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "pretrain_trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "base_model_evaluation = pretrain_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bcba2245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.9285240173339844,\n",
       " 'eval_accuracy': 0.5,\n",
       " 'eval_runtime': 4.1019,\n",
       " 'eval_samples_per_second': 259.876,\n",
       " 'eval_steps_per_second': 16.334}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Evaluation result before fine-tuning:\")\n",
    "base_model_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import  get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('gpt2',\n",
    "                                                      num_labels=2,\n",
    "                                                      id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "                                                      label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd514eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 814,080 || all params: 125,253,888 || trainable%: 0.6499438963523432\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "                    r=8,\n",
    "                    lora_alpha=32,\n",
    "                    target_modules=['c_attn', 'c_proj'],\n",
    "                    lora_dropout=0.1,\n",
    "                    bias=\"none\",\n",
    "                    task_type=TaskType.SEQ_CLS\n",
    "                )\n",
    "\n",
    "peft_model = get_peft_model(model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8158ef9833d746629ee97367429be22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Rename 'label' column to 'labels'\n",
    "tokenized_dataset[\"train\"] = tokenized_dataset[\"train\"].map(lambda e: {'labels': e['label']}, batched=True, remove_columns=['label'])\n",
    "tokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].map(lambda e: {'labels': e['label']}, batched=True, remove_columns=['label'])\n",
    "\n",
    "# Print to verify\n",
    "print(tokenized_dataset[\"train\"][0][\"text\"])\n",
    "print(tokenized_dataset[\"train\"][0][\"labels\"])\n",
    "\n",
    "# Set the format for PyTorch\n",
    "tokenized_dataset[\"train\"].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_dataset[\"test\"].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7928d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_peft = Trainer(\n",
    "model = peft_model,\n",
    "args = TrainingArguments(\n",
    "    output_dir = \"./lora_model_output\",\n",
    "    learning_rate = 2e-5,\n",
    "    logging_strategy = \"steps\",\n",
    "    per_device_train_batch_size = 12,\n",
    "    per_device_eval_batch_size = 12,\n",
    "    num_train_epochs = 1,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end = True\n",
    "),\n",
    "train_dataset = tokenized_dataset[\"train\"],\n",
    "eval_dataset = tokenized_dataset[\"test\"],\n",
    "tokenizer = tokenizer,\n",
    "data_collator =  DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc3eb948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='711' max='711' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [711/711 01:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>0.595125</td>\n",
       "      <td>0.733583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=711, training_loss=0.6522077085599618, metrics={'train_runtime': 113.829, 'train_samples_per_second': 74.937, 'train_steps_per_second': 6.246, 'total_flos': 330172861962240.0, 'train_loss': 0.6522077085599618, 'epoch': 1.0})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_peft.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(\"gpt-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "\n",
    "NUM_LABELS = 2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\"gpt-lora\", num_labels=NUM_LABELS, ignore_mismatched_sizes=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./data/sentiment_analysis\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "finetuned_trainer = Trainer(\n",
    "    model=lora_model,  # The fine-tuned PEFT model.\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df191a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for the fine-tuned model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5951250195503235,\n",
       " 'eval_accuracy': 0.7335834896810507,\n",
       " 'eval_runtime': 4.5234,\n",
       " 'eval_samples_per_second': 235.664,\n",
       " 'eval_steps_per_second': 14.812}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the fine-tuned model on the validation set\n",
    "finetuned_results = finetuned_trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results for the fine-tuned model\n",
    "print(\"Evaluation results for the fine-tuned model:\")\n",
    "finetuned_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401c81b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
